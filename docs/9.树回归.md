# 第9章 树回归
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![预测数值型数据回归首页](/images/9.TreeRegression/TreeRegression_headPage_xy.png "树回归首页")

## CART 算法

> CART 算法简介

```
    CART(Classification And Regression Trees，分类回归树)的树构建算法。该算法既可以用于分类还可以用于回归，因此非常值得学习。
    CART 是十分著名的且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对CART稍作修改就可以处理回归问题。
```

> CART与ID3的一些不同

```
    第三章使用决策树来进行分类。决策树不断将数据切分成小数据集，直到所有的目标变量完全相同，或者数据不能再切分为止。决策树是一种贪心算法，
它要在给定的时间内做出最佳选择，但并不关心能否达到全局最优。
    第3章使用的树构建算法是ID3.ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，
如果一个特征有4种取值，那么数据将被切成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所有有观点认为这种切分方式过于迅速。
另外一种方法是二元切分法，即每次把数据集切成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。
    除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在ID3算法中使用。
但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。
    上面说到了一点，对CART稍作修改就可以处理回归问题。第3章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。
```

## 回归树

> 树回归的特点

```
    优点：可以对复杂和非线性的数据建模。
    缺点：结果不易理解。
    适用数据类型：数值型和标称型数据。
```

> 树回归的一般方法

```
    (1) 收集数据：采用任意方法收集数据。
    (2) 准备数据：需要数值型数据，标称型数据应该映射成二值型数据。
    (3) 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树。
    (4) 训练算法：大部分时间都花费在叶节点树模型的构建上。
    (5) 测试算法：使用测试数据上的R^2值来分析模型的效果。
    (6) 使用算法：使用训练处的树做预测，预测结果还可以用来做很多事情。
```

> 连续和离散型特征的树的构建

```
    在树的构建过程中，需要解决多种类型数据的存储问题，这里将使用一部字典来存储树的数据结构，该字典将包含以下4个元素。
        * 待切分的特征
        * 待切分的特征值
        * 右子树。当不再需要切分的时候，也可以是单个值。
        * 左子树。与右子树类似。
```

下面我们要构建两种树：第一种是 回归树(regression tree)，其每个叶节点包含单个值；第二种是 模型树(model tree)，其每个叶节点包含一个线性方程。
下面给出两种树构建算法中的一些共用代码：
    函数createTree() 的伪代码大致如下：
    找到最佳的待切分特征：
        如果该节点不能再分，将该节点存为叶节点
        执行二元切分
        在右子树调用createTree()方法
        在左子树调用createTree()方法


> 将 CART 算法用于回归

```
    回归树假设叶节点是常数值，这种策略认为数据中的复杂关系可以用树结构来概括。
    为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给点节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？
事实上，在数据集上计算混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。
上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。
总方差可以通过均方差乘以数据集中样本点的个数来得到。
```

> 将回归树运用到下面的数据中，目标是从该数据生成一棵回归树

![CART构建数据集回归树的简单数据集](/images/9.TreeRegression/CART构建数据集回归树的简单数据集.png "CART构建数据集回归树的简单数据集")

![测试回归树的分段常数数据集](/images/9.TreeRegression/测试回归树的分段常数数据集.png "测试回归树的分段常数数据集")

## 模型树

    用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的分段线性(piecewise linear)是指模型由多个
先行片段组成。参考下图中的数据，如果使用两条直线拟合是否比使用一组常数来建模好呢？答案显而易见。可以设计两条分别从0.0~0.3、从0.3~1.0的直线，于是就可以
得到两个线性模型。因为数据集里的一部分数据（0.0~0.3）以某个线性模型建模，而另一部分数据（0.3~1.0）则以另一个线性模型建模，因此我们说采用了所谓的分段线性模型。
    决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。
另外，模型树也具有更高的预测准确度。

![模型树示例图](/images/9.TreeRegression/模型树示例图.png "模型树示例图")


## 树剪枝(tree pruning)算法

一棵树如果节点过多，表明该模型可能对数据进行了“过拟合”。那么，如何判断是否发生了过拟合？前面章节中使用了测试集上某种交叉验证技术来发现过拟合，决策树亦是如此。
通过降低决策树的复杂度来避免过拟合的过程称为“剪枝(pruning)”。

> 预剪枝

```
    本章前面已经进行过剪枝处理。在函数chooseBestSplit()中提前终止条件，实际上是在进行一种所谓的预剪枝(prepruning)操作。
```

> 后剪枝

```
    利用测试集来对树进行剪枝。由于不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。
    使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大，足够复杂便于剪枝。
    下面是从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低误差。如果是的话就合并。
    函数prune()的伪代码如下：
    基于已有的树切分测试数据：
        如果存在任一子集是一棵树，则在该子集递归剪枝过程
        计算将当前两个叶节点合并后的误差
        计算不合并的误差
        如果合并会降低误差的话，就将叶节点合并
```
## Python中GUI的使用

> 使用 Python 的 Tkinter 库创建 GUI

```
    如果能让用户不需要任何指令就可以按照他们自己的方式来分析数据，就不需要对数据做出过多解释。其中一个能同时支持数据呈现和用户交互的方式就是
构建一个图形用户界面(GUI，Graphical User Interface)，如图9-7所示。
```

![GUI示例图](/images/9.TreeRegression/GUI示例图.png "GUI示例图")

> 用 Tkinter 创建 GUI

```
    Python 有很多 GUI 框架，其中一个易于使用的 Tkinter，是随 Python 的标准版编译版本发布的。Tkinter 可以在 Windows、Mac OS和大多数的 Linux 平台上使用。
```

> 集成 Matplotlib 和 Tkinter

```
    MatPlotlib 的构建程序包含一个前端，也就是面向用户的一些代码，如 plot() 和 scatter() 方法等。事实上，它同时创建了一个后端，用于实现绘图和不同应用之间接口。
通过改变后端可以将图像绘制在PNG、PDF、SVG等格式的文件上。下面将设置后端为 TkAgg (Agg 是一个 C++ 的库，可以从图像创建光栅图)。TkAgg可以在所选GUI框架上调用Agg，
把 Agg 呈现在画布上。我们可以在Tk的GUI上放置一个画布，并用 .grid()来调整布局。
```

> 用treeExplore 的GUI构建的模型树示例图

![取得更好预测效果的GUI示例图](/images/9.TreeRegression/GUI更好的示例图.png "取得更好预测效果的GUI示例图")


## 树回归小结

    数据集中经常包含一些复杂的相关关系，使得输入数据和目标变量之间呈现非线性关系。对这些复杂的关系建模，一种可行的方式是使用树来对预测值分段，
包括分段常数或分段直线。一般采用树结构来对这种数据建模。相应地，若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型师线性回归方程则称为模型树。
    CART算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART算法构建模型树和回归树。该算法构建出的树会倾向于
对数据过拟合。一棵过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝（在树的构建过程中就进行剪枝）和后剪枝（当树
构建完毕再进行剪枝），预剪枝更有效但需要用户定义一些参数。
    Tkinter 是 Python 的一个 GUI 工具包。虽然并不是唯一的包，但它最常用。利用 Tkinter ，我们可以轻轻松松绘制各种部件并安排它们的位置。另外，可以为
Tkinter 构造一个特殊的部件来显示 Matplotlib 绘出的图。所以，Matplotlib 和 Tkinter 的集成可以构建出更强大的 GUI ，用户可以以更自然的方式来探索机器学习算法的奥妙。

* * *

* **作者：[片刻](http://www.apache.wiki/display/~jiangzhonglian) [小瑶](http://www.apache.wiki/display/~chenyao)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apachecn.org/)**
