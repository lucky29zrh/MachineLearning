# 第8章 预测数值型数据：回归
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![预测数值型数据回归首页](/images/8.Regression/预测数值型数据回归首页.png "回归Regression首页")

## 线性回归

> 回归简介

```
    回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。
    假如你想要预测姐姐男友汽车的功率大小，可能会这么计算：
    HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio
    这就是所谓的回归方程(regression equation)，其中的0.0015和-0.99称作回归系数(regression weights)，求这些回归系数的过程就是回归。
    一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。
    说到回归，一般都是指线性回归(linear regression)，所以本章里的回归和线性回归代表同一个意思。线性回归意味着可以将输入项分别乘以一些常量，
    再将结果加起来得到输出。需要说明的是，存在另一种称为非线性回归的回归模型，该模型不认同上面的做法，比如认为可能是输入的乘积。
    这样，上面的功率计算公式也可以写做：
    HorsePower = 0.0015 * annualSalary / hoursListeningToPublicRadio 
    这就是一个非线性回归的例子，但是对此不进行深入讨论。
```

> 线性回归特点

```
    优点：结果易于理解，计算上不复杂。
    缺点：对非线性的数据拟合不好。
    适用于数据类型：数值型和标称型数据。
```

> 回归的一般方法

```
    (1) 收集数据：采用任意方法收集数据。
    (2) 准备数据：回归需要数值型数据，标称型数据将被转换成二值型数据。
    (3) 分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比。
    (4) 训练算法：找到回归系数。
    (5) 测试算法：使用 R^2 或者预测值和数据的拟合度，来分析模型的效果。
    (6) 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。
```

> 线性回归的效果图

![线性回归效果图](/images/8.Regression/线性回归效果图.png "线性回归效果图")

## 局部加权线性回归

> 局部加权线性回归简介

```
    线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方误差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。
所以有些方法允许在估计中引入一些偏差，从而降低预测的均方差。
    其中的一个方法是局部加权线性回归(Locally Weighted Linear Regression, LWLR)。在该算法中，我们给带预测点附近的每个点赋予一定的权重，
在这个子集上基于最小均方差来进行普通的回归。与kNN一样，这种算法每次预测均需要实现选取出对应的数据子集。该算法解出回归系数w的形式如下：
    w = (X^T W X)^(-1)X^T Wy
其中 w 是一个矩阵，用来给每个数据点赋予权重。
```

> 局部加权线性回归效果图

![局部加权线性回归效果图](/images/8.Regression/局部加权线性回归效果图.png "局部加权线性回归效果图")


## 岭回归和逐步线性回归

> 缩减系数来“理解”数据

```
    如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即不能再使用前面介绍的方法。
这是因为在计算 (x^TX)^(-1) 的时候会出错。
    如果特征比样本点还多(n > m)，也就是说输入数据的矩阵X不是满秩矩阵。非满秩矩阵在求逆时会出现问题。
    为了解决这个问题，统计学家引入了岭回归(ridge regression)的概念，这就是本节将要介绍的第一种缩减方法。
```

> 岭回归

```
    简单来说，岭回归就是在矩阵 X^TX 上加一个 λI 从而使得矩阵非奇异，进而能对 X^TX+λI 求逆。其中矩阵I是一个 m*m 的单位矩阵，
    对角线上元素全为1，其他元素全为0。而λ是一个用户定义的数值，后面会做介绍。在这种情况下，回归系数的计算公式将变成：
        w = (X^TX + λI)^(-1)X^Ty
    岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入λ来限制了所有w之和，通过引入该惩罚项，
    能够减少不重要的参数，这个技术在统计学中也叫作缩减(shrinkage)。
```

> 岭回归中的岭是什么

```
    岭回归使用了单位矩阵乘以常量λ，我们观察其中的单位矩阵I，可以看到值1贯穿整个对角线，其余元素全是0.形象地，在0构成的平面上有一条1组成的“岭”，
    这就是岭回归中的“岭”的由来。
```
> 岭回归示例图

![岭回归示例图](/images/8.Regression/岭回归示例图.png "岭回归示例图")

> 前向逐步回归简介

```
    前向逐步回归算法可以得到与 lasso 差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。
    一开始，所有的权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。
```

> 前向逐步回归伪代码

```
    数据标准化，使其分布满足0均值和单位方差
    在每轮迭代过程中：
        设置当前最小误差lowestError为正无穷
        对每个特征：
            增大或缩小：
                改变一个系数得到一个新的w
                计算新w下的误差
                如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的w
            将W设置为新的Wbest
```

```
    贪心算法在所有特征上运行两次for循环，分别计算增加或减少该特征对误差的影响。这里使用的是平方误差，通过之前的函数rssError()得到。
    该误差初始值设为正无穷，经过与所有的误差比较后取最小的误差。整个过程循环迭代进行。
```

> 逐步线性回归示例图

![逐步线性回归示例图](/images/8.Regression/逐步线性回归示例图.png "逐步线性回归示例图")

## 预测鲍鱼年龄和玩具售价

> 示例：预测鲍鱼的年龄

```
    鲍鱼年龄可以从鲍鱼壳的层数推算得到。
    为了分析预测误差的大小，可以用函数rssError()计算出这一指标，下面是三种不同的预测误差：
    >>> regression.rssError(abY[0:99],yHat01.T)
    56.842594430533545
    >>> regression.rssError(abY[0:99],yHat1.T)
    429.89056187006685
    >>> regression.rssError(abY[0:99],yHat10.T)
    549.11817088257692
    可以看到，使用较小的核将得到较低的误差。那么，为什么不在所有数据集上都使用最小的核呢？
    这是因为使用最小的核将造成过拟合，对新数据不一定能达到最好的预测效果。

    简单线性回归达到了与局部线性回归类似的效果。这也表明一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是10吗？
    或许是，但如果想得到最好的效果，应该用10个不同的样本集做10次测试来比较结果。
```

> 示例：预测乐高玩具套装的价格

> 用回归法预测乐高套装的价格的一般过程

```
    (1) 收集数据：用 Google Shopping 的API收集数据。
    (2) 准备数据：从返回的JSON数据中抽取价格。
    (3) 分析数据：可视化并观察数据。
    (4) 训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型。
    (5) 测试算法：使用交叉验证来测试不同的模型，分析哪个效果最好。
    (6) 使用算法：这次练习的目标就是生成数据模型。
```

## 预测数值型数据：回归 小结

    与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型变量，而后者预测离散型变量。回归是统计学中最有力的的工具之一。
在回归方程里，求得特征对应的最佳回归系数的方法是最小化误差的平方和。给定输入矩阵x，如果 x^Tx的逆存在并可以求得的话，回归法都可以直接使用。
数据集上计算出的回归方程并不一定意味着它是最佳的，可以使用预测值yHat和原始值y的相关性来度量回归方程的好坏。
    当数据的样本数比特征数还少的时候，矩阵x^Tx的逆不能直接计算。即便当样本数比特征数多时，x^Tx的逆仍有可能无法直接计算，这是因为特征有可能高度相关。
这时可以考虑使用岭回归，因为当x^Tx的逆不能计算时，它仍保证能求得回归系数。
    岭回归是缩减法的一种，相当于对回归系数的大小施加了限制。另一种很好的缩减法是lasso。Lasso难以理解，但可以使用计算简便的逐步线性回归方法来求得近似结果。
    缩减法还可以看做是对一个模型增加偏差的同时减少方差。偏差方差折中是一个重要的概念，可以帮助我们理解现有模型并做出改进，从而得到最好的模型。

* * *

* **作者：[小瑶](http://www.apache.wiki/display/~chenyao) [片刻](http://www.apache.wiki/display/~jiangzhonglian)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apachecn.org/)**